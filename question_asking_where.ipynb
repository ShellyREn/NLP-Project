{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import spacy\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "textfile = \"data/set4/a7.txt\"\n",
    "#put entire text file into a list of sentences\n",
    "def get_text(textFile):\n",
    "\ttext = []\n",
    "\twith open(textFile, \"r\") as f:\n",
    "\t\tfor line in f:\n",
    "\t\t\ttext = text + sent_tokenize(line)\n",
    "\treturn text\n",
    "text = get_text(textfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [('h', 'X', 'LS', 'ROOT', False)], 1: [('e', 'VERB', 'VB', 'ROOT', False)], 2: [('l', 'NOUN', 'NN', 'ROOT', False)], 3: [('l', 'NOUN', 'NN', 'ROOT', False)], 4: [('o', 'INTJ', 'UH', 'ROOT', False)], 5: [(' ', 'SPACE', '_SP', 'ROOT', False)], 6: [('m', 'VERB', 'VBP', 'ROOT', False)], 7: [('y', 'PROPN', 'NNP', 'ROOT', False)], 8: [(' ', 'SPACE', '_SP', 'ROOT', False)], 9: [('n', 'ADV', 'RB', 'ROOT', False)], 10: [('a', 'DET', 'DT', 'ROOT', True)], 11: [('m', 'VERB', 'VBP', 'ROOT', False)], 12: [('e', 'VERB', 'VB', 'ROOT', False)], 13: [(' ', 'SPACE', '_SP', 'ROOT', False)], 14: [('i', 'PRON', 'PRP', 'ROOT', True)], 15: [('s', 'VERB', 'VBZ', 'ROOT', False)], 16: [(' ', 'SPACE', '_SP', 'ROOT', False)], 17: [('L', 'NOUN', 'NN', 'ROOT', False)], 18: [('a', 'DET', 'DT', 'ROOT', True)], 19: [('u', 'NOUN', 'NN', 'ROOT', False)], 20: [('r', 'NOUN', 'NN', 'ROOT', False)], 21: [('a', 'DET', 'DT', 'ROOT', True)]}\n"
     ]
    }
   ],
   "source": [
    "print(pos_tag_lst(\"hello my name is Laura\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (<ipython-input-6-855aa09a104e>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-855aa09a104e>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    print(doc)\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "def pos_tag_lst(text):\n",
    "\t#list of sentences\n",
    "\tPOS_tag_dict = dict()\n",
    "\tfor i,line in enumerate(text):\n",
    "\t\ttags = []\n",
    "\t\tdoc = nlp(str(line))\n",
    "        print(doc)\n",
    "\t\tfor token in doc:\n",
    "#             print(doc)\n",
    "#             print(token)\n",
    "\t\t\ttags.append((token.text, token.pos_, token.tag_, token.dep_, token.is_stop))\n",
    "\t\tif len(tags) != 0:\n",
    "\t\t\tPOS_tag_dict[i] = tagsb\n",
    "\treturn POS_tag_dict\n",
    "\n",
    "def pos_tag_sentence(sentence):\n",
    "\t#list of sentences\n",
    "\tPOS_tag_dict = dict()\n",
    "\ttext = sentence.split()\n",
    "\tfor line in text:\n",
    "\t\ttags = []\n",
    "\t\tdoc = nlp(str(line))\n",
    "\t\tfor token in doc:\n",
    "\t\t\ttags.append((token.pos_, token.tag_, token.dep_, token.is_stop, ))\n",
    "\t\tif len(tags) != 0:\n",
    "\t\t\tPOS_tag_dict[token.text] = tags[0]\n",
    "\treturn POS_tag_dict\n",
    "\n",
    "#Token dict\n",
    "def dependency_dict(doc):\n",
    "\tout = dict()\n",
    "\troot = ''\n",
    "\tfor token in doc:\n",
    "\t\tout[token.text] = (token.dep_, token.head.text, token.head.pos_,[child for child in token.children])\n",
    "\t\tif token.dep_ == \"ROOT\":\n",
    "\t\t\troot = token.text\n",
    "\treturn out, root\n",
    "\n",
    "# NER tagging\n",
    "def ner_tag(text):\n",
    "\tNER_tag_dict = dict()\n",
    "\tfor i,line in enumerate(text):\n",
    "\t\ttags = []\n",
    "\t\tdoc = nlp(str(line))\n",
    "\n",
    "\t\tfor ent in doc.ents:\n",
    "\t\t\t# print(ent.text +'-' + ent.label_ + '\\n')\n",
    "\t\t\ttags.append(ent.text +'-' + ent.label_)\n",
    "\t\tif len(tags) != 0:\n",
    "\t\t\tNER_tag_dict[i] = tags\n",
    "\treturn NER_tag_dict\n",
    "\n",
    "def ner_tag_sentence(sentence):\n",
    "\tdoc = nlp(str(sentence))\n",
    "\tNER_tag_dict = dict()\n",
    "\ttags = []\n",
    "\tfor ent in doc.ents:\n",
    "\t\t# print(ent.text +'-' + ent.label_ + '\\n')\n",
    "\t\tNER_tag_dict[ent.text] = ent.label_\n",
    "\treturn NER_tag_dict\n",
    "\n",
    "\n",
    "#check tense of verb\n",
    "def check_tense(root, pos_dict):\n",
    "\ttag = pos_dict[root][1]\n",
    "\tif tag == \"VB\":\n",
    "\t\treturn \"do\"\n",
    "\telif tag == \"VBD\":\n",
    "\t\treturn \"did\"\n",
    "\telif tag == \"VBG\":\n",
    "\t\treturn \"doing\"\n",
    "\telif tag == \"VBN\":\n",
    "\t\treturn \"done\"\n",
    "\telif tag == \"VBP\":\n",
    "\t\treturn \"do\"\n",
    "\telif tag == \"VBZ\":\n",
    "\t\treturn \"does\"\n",
    "\telse:\n",
    "\t\treturn None\n",
    "\n",
    "# return dictionary of lemmas in sentence\n",
    "def getTokenLemma(nlp_doc):\n",
    "\tlemmas = {}\n",
    "\tfor token in nlp_doc:\n",
    "\t\tlemmas[str(token)] = token.lemma_\n",
    "\treturn lemmas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pos_tag_sentence(sentence):\n",
    "#     #list of sentences\n",
    "#     POS_tag_dict = dict()\n",
    "#     text = sentence.split()\n",
    "#     for i,line in enumerate(text):\n",
    "#         tags = []\n",
    "#         doc = nlp(str(line))\n",
    "#         for token in doc:\n",
    "#             tags.append((token.text, token.pos_, token.tag_, token.dep_, token.is_stop, ))\n",
    "#         if len(tags) != 0:\n",
    "#             POS_tag_dict[i] = tags\n",
    "#     return POS_tag_dict\n",
    "def pos_tag_sentence(sentence):\n",
    "\t# list of sentences\n",
    "\tPOS_tag_dict = dict()\n",
    "\ttext = sentence.split()\n",
    "\tfor line in text:\n",
    "\t\ttags = []\n",
    "\t\tdoc = nlp(str(line))\n",
    "\t\tfor token in doc:\n",
    "\t\t\ttags.append((token.pos_, token.tag_, token.dep_, token.is_stop,))\n",
    "\t\tif len(tags) != 0:\n",
    "\t\t\tPOS_tag_dict[token.text] = tags[0]\n",
    "\treturn POS_tag_dict\n",
    "\n",
    "def pos_tag_lst(text):\n",
    "    #list of sentences\n",
    "    POS_tag_dict = dict()\n",
    "    for i,line in enumerate(text):\n",
    "        tags = []\n",
    "        doc = nlp(str(line))\n",
    "        for token in doc:\n",
    "            tags.append((token.text, token.pos_, token.tag_, token.dep_, token.is_stop))\n",
    "        if len(tags) != 0:\n",
    "            POS_tag_dict[i] = tags\n",
    "    return POS_tag_dict\n",
    "\n",
    "#Token dict \n",
    "def dependency_dict(doc):\n",
    "    out = dict()\n",
    "    root = ''\n",
    "    for token in doc:\n",
    "        out[token.text] = (token.dep_, token.head.text, token.head.pos_,[child for child in token.children])\n",
    "        if token.dep_ == \"ROOT\":\n",
    "            root = token.text\n",
    "    return out, root\n",
    "\n",
    "def ner_tag_sentence(sentence):\n",
    "    doc = nlp(str(sentence))\n",
    "    NER_tag_dict = dict()\n",
    "    tags = []\n",
    "    for ent in doc.ents:\n",
    "        # print(ent.text +'-' + ent.label_ + '\\n')\n",
    "        NER_tag_dict[ent.text] = ent.label_\n",
    "    return NER_tag_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HowOftenQ(sentence, ner_tag_dict, dependency_dict, root):\n",
    "    date = \"\"\n",
    "    output = \"\"\n",
    "    clause = \"\"\n",
    "    sentence = sentence[:-1]\n",
    "    for k in ner_tag_dict.keys():\n",
    "        if ner_tag_dict[k] == 'DATE':\n",
    "            date = k\n",
    "    if date == \"\": return\n",
    "    date_lst = date.split()\n",
    "    sentence_lst = sentence.split()\n",
    "    root_lst_ind = sentence_lst.index(root)\n",
    "    root_ind = sentence.index(root)\n",
    "    date_ind = sentence.index(date)\n",
    "    if date_ind < root_ind:\n",
    "        word_in_front_of_root = sentence_lst[root_lst_ind - 1]\n",
    "        if token_Dict[word_in_front_of_root][0] == 'auxpass':\n",
    "            sentence_lst[root_lst_ind - 1] = sentence_lst[root_lst_ind - 2]\n",
    "            sentence_lst[root_lst_ind - 2] = word_in_front_of_root\n",
    "        else:\n",
    "            sentence_lst[root_lst_ind] = nlp(root)[0].lemma_\n",
    "            if pos_tag_sentence(root)[0][0][2] in (\"VBD\", \"VBN\"):\n",
    "                sentence_lst.insert(root_lst_ind-1,\"did\")\n",
    "            else: sentence_lst.insert(root_lst_ind-1,\"does\")\n",
    "        for i in date_lst:\n",
    "            sentence_lst.remove(i)\n",
    "        output = \"How long \" + \" \".join(sentence_lst)\n",
    "    else:\n",
    "        for seg in sentence.split(',', 1):\n",
    "            if date in seg: clause = seg\n",
    "        if root_ind != 0:\n",
    "            word_in_front_of_root = sentence_lst[root_lst_ind - 1] \n",
    "            word_after_root = sentence_lst[root_lst_ind + 1]\n",
    "            prep_ind = sentence.index(word_after_root)\n",
    "            if token_Dict[word_after_root][0] != 'prep': word_after_root = \"\"\n",
    "            #if it's passive tense\n",
    "            if token_Dict[word_in_front_of_root][0] == 'auxpass':\n",
    "                output = \"How long \" + word_in_front_of_root + \" \" + clause.split(word_in_front_of_root,1)[0].lower() +  sentence[root_ind:prep_ind+len(word_after_root)]\n",
    "            else:\n",
    "                #if it's past tense\n",
    "                if pos_tag_sentence(root)[0][0][2] in (\"VBD\", \"VBN\"):\n",
    "                    output = \"How long\" + \" did \" + clause.split(root,1)[0].lower() + nlp(root)[0].lemma_ + sentence[root_ind+len(root):prep_ind+len(word_after_root)]\n",
    "                #if it's not past tense\n",
    "                else:\n",
    "                    output = \"How long\" + \" does \" + clause.split(root,1)[0].lower() + nlp(root)[0].lemma_ + sentence[root_ind+len(root):prep_ind+len(word_after_root)]\n",
    "    output = ' '.join(output.split()) + \"?\"\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Four months after Harris's death, Cuarón chose Gambon as his replacement.\"\n",
    "#sentence = \"With Prisoner of Azkaban, production of the Harry Potter films was switched to an eighteen-month cycle.\"\n",
    "# sentence = \"Harry Potter and the Prisoner of Azkaban is a 2004 fantasy film directed by Alfonso Cuaron and distributed by Warner Bros. Pictures.\"\n",
    "doc = nlp(sentence)\n",
    "token_Dict, root = dependency_dict(doc)\n",
    "ner_tag_dict = ner_tag_sentence(sentence)\n",
    "pos_tag_dict = pos_tag_sentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Four': ('NUM', 'CD', 'ROOT', True),\n",
       " 'months': ('NOUN', 'NNS', 'ROOT', False),\n",
       " 'after': ('ADP', 'IN', 'ROOT', True),\n",
       " \"'s\": ('PROPN', 'NNP', 'ROOT', False),\n",
       " ',': ('NOUN', 'NN', 'ROOT', False),\n",
       " 'Cuarón': ('PROPN', 'NNP', 'ROOT', False),\n",
       " 'chose': ('VERB', 'VBD', 'ROOT', False),\n",
       " 'Gambon': ('PROPN', 'NNP', 'ROOT', False),\n",
       " 'as': ('ADP', 'IN', 'ROOT', True),\n",
       " 'his': ('PRON', 'PRP$', 'ROOT', True),\n",
       " '.': ('NOUN', 'NN', 'ROOT', False)}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [('is', 'AUX', 'VBZ', 'ROOT', True)]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag_sentence(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [('Four', 'NUM', 'CD', 'ROOT', True)],\n",
       " 1: [('months', 'NOUN', 'NNS', 'ROOT', False)],\n",
       " 2: [('after', 'ADP', 'IN', 'ROOT', True)],\n",
       " 3: [('Harris', 'PROPN', 'NNP', 'ROOT', False),\n",
       "  (\"'s\", 'PART', 'POS', 'case', True)],\n",
       " 4: [('death', 'NOUN', 'NN', 'ROOT', False),\n",
       "  (',', 'PUNCT', ',', 'punct', False)],\n",
       " 5: [('Cuarón', 'PROPN', 'NNP', 'ROOT', False)],\n",
       " 6: [('chose', 'VERB', 'VBD', 'ROOT', False)],\n",
       " 7: [('Gambon', 'PROPN', 'NNP', 'ROOT', False)],\n",
       " 8: [('as', 'ADP', 'IN', 'ROOT', True)],\n",
       " 9: [('his', 'PRON', 'PRP$', 'ROOT', True)],\n",
       " 10: [('replacement', 'NOUN', 'NN', 'ROOT', False),\n",
       "  ('.', 'PUNCT', '.', 'punct', False)]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag_sentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How long does harry potter and the prbe?'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HowOftenQ(sentence, ner_tag_dict, token_Dict, root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whereQ(sentence, dep_dict, root):\n",
    "    output = ''\n",
    "    verbs = ['was', 'is', 'were']\n",
    "    foundVerb = False\n",
    "    foundVerbInd = 0\n",
    "    # find tense \n",
    "    pos_tags = pos_tag_sentence(sentence)\n",
    "    seenWhere = False\n",
    "    foundWhereInd = 0\n",
    "    whereInd = 0\n",
    "    tense = None\n",
    "    for word in sentence.split():\n",
    "        if word == 'where':\n",
    "            seenWhere = True\n",
    "            foundWhereInd = whereInd\n",
    "            verb = dep_dict[word][1]\n",
    "            if pos_tags[verb][0] == 'VERB':\n",
    "                tense = check_tense(verb, pos_tags)\n",
    "            elif verb in verbs: # if tense is was, is, were\n",
    "                tense = verb\n",
    "                foundVerb = True\n",
    "                foundVerbInd = sentence.split().index(verb)\n",
    "            break\n",
    "        whereInd += 1\n",
    "    if tense == None: # not able to create question from sentence\n",
    "        return \"No question\"\n",
    "    output += f'Where {tense} '\n",
    "    ind = foundWhereInd\n",
    "    if foundVerb:\n",
    "        ind = foundVerbInd\n",
    "    for word in sentence.split()[ind+1:]:\n",
    "        output += word + \" \"    \n",
    "    return output[:-2] + \"?\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'This': ('nsubj', 'leads', 'VERB', []), 'leads': ('ROOT', 'leads', 'VERB', [This, trio, to, .]), 'the': ('det', 'dog', 'NOUN', []), 'trio': ('dobj', 'leads', 'VERB', [the]), 'to': ('prep', 'passage', 'NOUN', [Shack]), 'an': ('det', 'Animagus', 'PROPN', []), 'underground': ('amod', 'passage', 'NOUN', []), 'passage': ('pobj', 'to', 'ADP', [an, underground, to]), 'Shrieking': ('compound', 'Shack', 'PROPN', []), 'Shack': ('pobj', 'to', 'ADP', [the, Shrieking, ,, discover]), ',': ('punct', 'Sirius', 'PROPN', []), 'where': ('advmod', 'discover', 'VERB', []), 'they': ('nsubj', 'discover', 'VERB', []), 'discover': ('relcl', 'Shack', 'PROPN', [where, they, is]), 'that': ('mark', 'is', 'AUX', []), 'dog': ('nsubj', 'is', 'AUX', [the]), 'is': ('relcl', 'Sirius', 'PROPN', [who, Animagus]), 'actually': ('advmod', 'is', 'AUX', []), 'Sirius': ('attr', 'is', 'AUX', [,, is]), 'who': ('nsubj', 'is', 'VERB', []), 'Animagus': ('attr', 'is', 'VERB', [an]), '.': ('punct', 'leads', 'VERB', [])} leads\n"
     ]
    }
   ],
   "source": [
    "sentence = \"This leads the trio to an underground passage to the Shrieking Shack, where they discover that the dog is actually Sirius, who is an Animagus.\"\n",
    "dep_dict1, root1 = dependency_dict(nlp(sentence))\n",
    "print(dep_dict1, root1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'He': ('nsubj', 'was', 'AUX', []), 'was': ('ROOT', 'was', 'AUX', [He, in, .]), 'in': ('prep', 'was', 'AUX', [underground]), 'the': ('det', 'underground', 'NOUN', []), 'underground': ('pobj', 'in', 'ADP', [the, ,, were]), ',': ('punct', 'underground', 'NOUN', []), 'where': ('advmod', 'were', 'VERB', []), 'there': ('expl', 'were', 'VERB', []), 'were': ('relcl', 'underground', 'NOUN', [where, there, lots]), 'lots': ('attr', 'were', 'VERB', [of]), 'of': ('prep', 'lots', 'NOUN', [tea]), 'tea': ('pobj', 'of', 'ADP', []), '.': ('punct', 'was', 'AUX', [])} was\n"
     ]
    }
   ],
   "source": [
    "sentence2 = \"He was in the underground, where there were lots of tea.\"\n",
    "dep_dict2, root2 = dependency_dict(nlp(sentence2))\n",
    "print(dep_dict2, root2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'He': ('PRON', 'PRP', 'ROOT', True),\n",
       " 'was': ('AUX', 'VBD', 'ROOT', True),\n",
       " 'in': ('ADP', 'IN', 'ROOT', True),\n",
       " 'the': ('DET', 'DT', 'ROOT', True),\n",
       " ',': ('ADV', 'RB', 'ROOT', False),\n",
       " 'where': ('ADV', 'WRB', 'ROOT', True),\n",
       " 'there': ('ADV', 'RB', 'ROOT', True),\n",
       " 'were': ('AUX', 'VBD', 'ROOT', True),\n",
       " 'lots': ('NOUN', 'NNS', 'ROOT', False),\n",
       " 'of': ('ADP', 'IN', 'ROOT', True),\n",
       " '.': ('NOUN', 'NN', 'ROOT', False)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag_sentence(sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This': ('DET', 'DT', 'ROOT', True),\n",
       " 'leads': ('VERB', 'VBZ', 'ROOT', False),\n",
       " 'the': ('DET', 'DT', 'ROOT', True),\n",
       " 'trio': ('NOUN', 'NN', 'ROOT', False),\n",
       " 'to': ('PART', 'TO', 'ROOT', True),\n",
       " 'an': ('DET', 'DT', 'ROOT', True),\n",
       " 'underground': ('ADV', 'RB', 'ROOT', False),\n",
       " 'passage': ('NOUN', 'NN', 'ROOT', False),\n",
       " 'Shrieking': ('VERB', 'VBG', 'ROOT', False),\n",
       " ',': ('PROPN', 'NNP', 'ROOT', False),\n",
       " 'where': ('ADV', 'WRB', 'ROOT', True),\n",
       " 'they': ('PRON', 'PRP', 'ROOT', True),\n",
       " 'discover': ('VERB', 'VB', 'ROOT', False),\n",
       " 'that': ('DET', 'DT', 'ROOT', True),\n",
       " 'dog': ('NOUN', 'NN', 'ROOT', False),\n",
       " 'is': ('AUX', 'VBZ', 'ROOT', True),\n",
       " 'actually': ('ADV', 'RB', 'ROOT', False),\n",
       " 'who': ('PRON', 'WP', 'ROOT', True),\n",
       " '.': ('PROPN', 'NNP', 'ROOT', False)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag_sentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where do they discover that the dog is actually Sirius, who is an Animagus?\n"
     ]
    }
   ],
   "source": [
    "print(whereQ(sentence, dep_dict1, root1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where were lots of tea?\n"
     ]
    }
   ],
   "source": [
    "print(whereQ(sentence2, dep_dict2, root2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
